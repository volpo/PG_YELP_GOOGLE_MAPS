{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark import SparkConf\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::170663929988:role/GlueS3_ETL\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: f8830531-921d-4f04-9507-c19fa33713e7\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.38.1\n--enable-glue-datacatalog true\nWaiting for session f8830531-921d-4f04-9507-c19fa33713e7 to get into ready status...\nSession f8830531-921d-4f04-9507-c19fa33713e7 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Import the tables as Dynamic frame",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "google_stores_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"google_raw_metadata_sitios\",    \n)\n\nreviews_california_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"california_raw_review_california\",    \n)\n\nreviews_florida_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"florida_raw_review_florida\",    \n)\n\nreviews_nevada_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"nevada_rawreview_nevada\",    \n)\n\nreviews_newyork_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"ny_raw_review_new_york\",    \n)\n\nreviews_texas_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"texas_raw_review_texas\",    \n)\n\nyelp_stores_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"yelp_raw_business_parquet\",    \n)\n\nyelp_reviews_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"yelp_raw_review_001_json\",    \n)\n\nyelp_clientes_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"yelp_raw_user_003_parquet\",    \n)\n\nstates_dynamic = glueContext.create_dynamic_frame.from_catalog(\n    database=\"stores_and_yelp\",\n    table_name=\"yelp_raw_states_parquet\",    \n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Convert the dynamic frames into spark dataframes",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "google_stores_spark = google_stores_dynamic.toDF()\nreviews_california_spark = reviews_california_dynamic.toDF()\nreviews_florida_spark = reviews_florida_dynamic.toDF()\nreviews_nevada_spark = reviews_nevada_dynamic.toDF()\nreviews_newyork_spark = reviews_newyork_dynamic.toDF()\nreviews_texas_spark = reviews_texas_dynamic.toDF()\nyelp_stores_spark = yelp_stores_dynamic.toDF()\nyelp_reviews_spark = yelp_reviews_dynamic.toDF()\nyelp_clientes_spark = yelp_clientes_dynamic.toDF()\nstates_spark = states_dynamic.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# rename the column state_id to stateID in states_spark\n\nstates_spark = states_spark.withColumnRenamed('state_id', 'stateID')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import concat, concat_ws, col, row_number, lower, udf, split, array, create_map, array_contains, when, lit, monotonically_increasing_id, from_unixtime, explode\nfrom functools import reduce\nfrom pyspark.sql.types import StringType, TimestampType, ArrayType, IntegerType",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# YELP",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# First, the stores_yelp df is transform so it´ll fit the needs for the machine learning and data analysis stage\n\n\nstates = ['CA', 'FL', 'NV', 'NY', 'TX']\ncategories = ['restaurant', 'coffee', 'bar', 'deli', 'sandwich', 'grocery', 'bakery', 'restaurants', 'coffees', 'bars', 'delis', 'sandwiches', 'groceries', 'bakeries']\n\n# Drop duplicates, create the address column and filter the stores by state\n\nyelp_stores_spark = yelp_stores_spark.dropDuplicates(['business_id'])\nyelp_stores_spark = yelp_stores_spark.withColumn('address_total', concat(col('address'), col('city'), col('state'), col('postal_code')))\nyelp_stores_spark = yelp_stores_spark.drop('address', 'city', 'postal_code', 'is_open', 'review_count', 'hours', '__index_level_0__', 'stars')\nyelp_stores_spark = yelp_stores_spark.withColumnRenamed('address_total', 'address')\nyelp_stores_spark = yelp_stores_spark.filter(col('state').isin(states))\nyelp_stores_spark = yelp_stores_spark.withColumn('state', when(yelp_stores_spark['state'] == 'CA', 'California')\n                                                         .when(yelp_stores_spark['state'] == 'FL', 'Florida')\n                                                         .when(yelp_stores_spark['state'] == 'NV', 'Nevada')\n                                                         .when(yelp_stores_spark['state'] == 'NY', 'New York')\n                                                         .when(yelp_stores_spark['state'] == 'TX', 'Texas'))\n\n# Filter the stores by category and join with the states df to have the correct stateID\n\nyelp_stores_spark = yelp_stores_spark.withColumn('categories', lower(col('categories').cast('string')))\nyelp_stores_spark = yelp_stores_spark.withColumn('category_array', split(col('categories'), ','))\ncategories_expr = [array_contains(col('category_array'), cat) for cat in categories]\nyelp_stores_spark = yelp_stores_spark.withColumn('category_match', reduce(lambda x, y: x | y, categories_expr))\nyelp_stores_spark = yelp_stores_spark.filter(col('category_match'))\nyelp_stores_spark = yelp_stores_spark.join(states_spark, 'state', 'inner')\n\n# Create the column delivery and drop the unnecessary columns\n\nyelp_stores_spark = yelp_stores_spark.withColumn('delivery', col('attributes.RestaurantsDelivery'))\nyelp_stores_spark = yelp_stores_spark.withColumn('delivery', when(yelp_stores_spark['delivery'] == 'True', 'Y').otherwise('N'))\nyelp_stores_spark = yelp_stores_spark.drop('attributes', 'category_array', 'category_match', 'state')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Next, we need to transform the reviews_yelp df.\n# Drop duplicates, join with the stores df so the reviews from other stores won´t be taken into account, drop the unnecessary columns, and transform the\n# date column\n\nyelp_reviews_spark = yelp_reviews_spark.dropDuplicates(['review_id'])\nyelp_reviews_spark = yelp_reviews_spark.join(yelp_stores_spark, 'business_id', 'inner')\nyelp_reviews_spark = yelp_reviews_spark.drop('funny', 'cool', 'useful', 'name', 'address', 'latitude', 'longitude', 'stateID', 'delivery', 'categories')\nyelp_reviews_spark = yelp_reviews_spark.withColumnRenamed('stars', 'rating')\nyelp_reviews_spark = yelp_reviews_spark.withColumn(\"date\", from_unixtime(col(\"date\") / 1000).cast(TimestampType()))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Finally, we´ll transform the clientes_yelp df.\n# We drop the unnecessary columns and duplicates and join with the review_yelp df so only the users that made a review in the stores that\n# are being considered will be taken into account\n\ndrop_columns = ['review_count', 'yelping_since', 'useful', 'funny', 'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot', 'compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', 'compliment_funny', 'compliment_writer', 'compliment_photos']\nyelp_clientes_spark = yelp_clientes_spark.drop(*drop_columns)\nyelp_clientes_spark = yelp_clientes_spark.dropDuplicates(['user_id'])\nyelp_clientes_spark = yelp_clientes_spark.join(yelp_reviews_spark, 'user_id', 'inner')\nyelp_clientes_spark = yelp_clientes_spark.drop('business_id', 'review_id', 'rating', 'text', 'date')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Rename the ID columns in the yelps df\n\nyelp_stores_spark = yelp_stores_spark.withColumnRenamed('business_id', 'storeID')\nyelp_reviews_spark = yelp_reviews_spark.withColumnRenamed('business_id', 'storeID') \\\n                                       .withColumnRenamed('review_id', 'reviewID') \\\n                                       .withColumnRenamed('user_id', 'userID')\nyelp_clientes_spark = yelp_clientes_spark.withColumnRenamed('user_id', 'userID')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Google",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#It´s the turn for the google df.\n# First stores_google: drop the unnecessary columns, drop null values and convert to lower case all the words in the category column\n\ncategories = ['restaurant', 'coffee', 'bar', 'deli', 'sandwich', 'grocery', 'bakery', 'restaurants', 'coffees', 'bars', 'delis', 'sandwiches', 'groceries', 'bakeries']\ndrop_columns = ['description', 'state', 'price', 'hours', 'avg_rating', 'num_of_reviews', 'relative_results', 'url']\ndelivery = ['Delivery', 'delivery']\n\ngoogle_stores_spark = google_stores_spark.drop(*drop_columns)\ngoogle_stores_spark = google_stores_spark.dropDuplicates(['gmap_id'])\ngoogle_stores_spark = google_stores_spark.na.drop(subset = ['category', 'gmap_id'])\ngoogle_stores_spark = google_stores_spark.withColumn('category', concat_ws(', ', google_stores_spark['category']))\ngoogle_stores_spark = google_stores_spark.withColumn('category', lower(col('category').cast('string')))\n\n# Filter the categories\n\ngoogle_stores_spark = google_stores_spark.withColumn('category_array', split(col('category'), ','))\ncategories_expr = [array_contains(col('category_array'), cat) for cat in categories]\ngoogle_stores_spark = google_stores_spark.withColumn('category_match', reduce(lambda x, y: x | y, categories_expr))\ngoogle_stores_spark = google_stores_spark.filter(col('category_match'))\n\n# Convert the latitude and longitude columns to double\n\ngoogle_stores_spark = google_stores_spark.withColumn('latitude', col('latitude').getField('double'))\ngoogle_stores_spark = google_stores_spark.withColumn('longitude', col('longitude').getField('double'))\n\n# Create the delivery column and drop some unnecessary columns\n\ngoogle_stores_spark = google_stores_spark.withColumn(\"service_options\", col(\"MISC.Service options\"))\ngoogle_stores_spark = google_stores_spark.withColumn(\"delivery\", when(array_contains(col(\"service_options\"), delivery[0]) | array_contains(col(\"service_options\"), delivery[1]), \"Y\").otherwise(\"N\"))\ngoogle_stores_spark = google_stores_spark.drop('category_array', 'category_match', 'MISC', 'service_options')\n\n# Rename the category column to categories\n\ngoogle_stores_spark = google_stores_spark.withColumnRenamed(\"category\", \"categories\")                                        ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Add the column state with the name of the state, renamed the column name to user_name and time to date drop the rows where\n# text values are null\n\nreviews_california_spark = reviews_california_spark.withColumnRenamed(\"name\", \"user_name\") \\\n                                                    .withColumnRenamed(\"time\", \"date\")\nreviews_california_spark = reviews_california_spark.withColumn(\"state\", lit(\"California\"))\nreviews_california_spark = reviews_california_spark.na.drop(subset = ['text', 'gmap_id'])\n\nreviews_florida_spark = reviews_florida_spark.withColumnRenamed(\"name\", \"user_name\") \\\n                                            .withColumnRenamed(\"time\", \"date\")\nreviews_florida_spark = reviews_florida_spark.withColumn(\"state\", lit(\"Florida\"))\nreviews_florida_spark = reviews_florida_spark.na.drop(subset = ['text', 'gmap_id'])\n\nreviews_nevada_spark = reviews_nevada_spark.withColumnRenamed(\"name\", \"user_name\") \\\n                                            .withColumnRenamed(\"time\", \"date\")\nreviews_nevada_spark = reviews_nevada_spark.withColumn(\"state\", lit(\"Nevada\"))\nreviews_nevada_spark = reviews_nevada_spark.na.drop(subset = ['text', 'gmap_id'])\n\nreviews_texas_spark = reviews_texas_spark.withColumnRenamed(\"name\", \"user_name\") \\\n                                            .withColumnRenamed(\"time\", \"date\")\nreviews_texas_spark = reviews_texas_spark.withColumn(\"state\", lit(\"Texas\"))\nreviews_texas_spark = reviews_texas_spark.na.drop(subset = ['text', 'gmap_id'])\n\nreviews_newyork_spark = reviews_newyork_spark.withColumnRenamed(\"name\", \"user_name\") \\\n                                            .withColumnRenamed(\"time\", \"date\")\nreviews_newyork_spark = reviews_newyork_spark.withColumn(\"state\", lit(\"New York\"))\nreviews_newyork_spark = reviews_newyork_spark.na.drop(subset = ['text', 'gmap_id'])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Join the reviews df with the stores df\n\ncalifornia = google_stores_spark.join(reviews_california_spark, \"gmap_id\", \"inner\")\nflorida = google_stores_spark.join(reviews_florida_spark, \"gmap_id\", \"inner\")\nnevada = google_stores_spark.join(reviews_nevada_spark, \"gmap_id\", \"inner\")\nnewyork = google_stores_spark.join(reviews_newyork_spark, \"gmap_id\", \"inner\")\ntexas = google_stores_spark.join(reviews_texas_spark, \"gmap_id\", \"inner\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Unified the reviews df\n\nreviews_google = california.union(florida).union(nevada).union(newyork).union(texas)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create the df for stores and reviews of google\n\nstores_google = reviews_google.select('gmap_id', 'name', 'latitude', 'longitude', 'categories', 'address', 'delivery', 'state')\nreviews_google = reviews_google.drop('name', 'address', 'latitude', 'longitude', 'categories', 'delivery', 'pics', 'state')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Add the column review_id to reviews_google and rename the column user_name. Then, convert the date column type to timestamp\n\nreviews_google = reviews_google.withColumnRenamed(\"user_name\", \"name\").withColumnRenamed('gmap_id', 'storeID').withColumnRenamed('user_id', 'userID')\nreviews_google = reviews_google.withColumn(\"reviewID\", (monotonically_increasing_id() + 1).cast(\"int\"))\nreviews_google = reviews_google.withColumn('reviewID', col('reviewID').cast('string'))\n\nreviews_google = reviews_google.withColumn(\"date\", from_unixtime(col(\"date\") / 1000).cast(TimestampType()))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create a new table named clientes and another named comentarios. Make the transformation needed in both tables.\n# Then drop the columns name and resp from reviews_google\n\nclientes_google = reviews_google.select(\"name\", \"userID\")\nclientes_google = clientes_google.dropDuplicates(['userID'])\n\ncomentarios = reviews_google.select(\"resp\", \"reviewID\")\ncomentarios = comentarios.na.drop(subset = ['resp'])\ncomentarios = comentarios.withColumn(\"commentID\", (monotonically_increasing_id() + 1).cast(\"int\"))\ncomentarios = comentarios.withColumn(\"date\", col(\"resp.time\"))\ncomentarios = comentarios.withColumn(\"response\", col(\"resp.text\"))\ncomentarios = comentarios.withColumn(\"date\", from_unixtime(col(\"date\") / 1000).cast(TimestampType()))\ncomentarios = comentarios.drop('resp')\n\nreviews_google = reviews_google.drop(\"name\", \"resp\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Drop the duplicates from the column gmap_id of stores_google, renamed the column to storeID and join with states_spark\n\nstores_google = stores_google.withColumnRenamed(\"gmap_id\", \"storeID\")\nstores_google = stores_google.join(states_spark, \"state\", \"inner\")\nstores_google = stores_google.drop(\"state\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Union of the yelp and google dataframes",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Unify the yelp dataframes with the google dataframes\n\nstores = yelp_stores_spark.union(stores_google)\nreviews = yelp_reviews_spark.union(reviews_google)\nclientes = yelp_clientes_spark.union(clientes_google)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create the table category\n\nstores = stores.withColumn('categories', split(col('categories'), ','))\ncategory = stores.select(explode(\"categories\").alias(\"categories\"))\ncategory = category.dropDuplicates(['categories'])\ncategory = category.withColumn(\"categoryID\", (monotonically_increasing_id() + 1).cast(\"int\"))\ncategory = category.withColumnRenamed(\"categories\", \"category\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create the table store_category\n\nstore_category = stores.select('storeID', 'categories')\nstore_category = store_category.withColumn('categories', explode('categories'))\nstore_category = store_category.withColumnRenamed('categories', 'category')\nstore_category = store_category.join(category, \"category\", \"inner\")\nstore_category = store_category.drop('category')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# drop the column categories from stores\n\nstores = stores.drop('categories')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Upload the dataframes to S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Convert the spark df to dynamic frames\n\nfrom awsglue.dynamicframe import DynamicFrame\n\nstores_dynamic = DynamicFrame.fromDF(stores, glueContext, 'stores')\nreviews_dynamic = DynamicFrame.fromDF(reviews, glueContext, 'reviews')\nclientes_dynamic = DynamicFrame.fromDF(clientes, glueContext, 'clientes')\ncomentarios_dynamic = DynamicFrame.fromDF(comentarios, glueContext, 'comentarios')\nstates_dynamic = DynamicFrame.fromDF(states_spark, glueContext, 'states')\ncategory_dynamic = DynamicFrame.fromDF(category, glueContext, 'category')\nstore_category_dynamic = DynamicFrame.fromDF(store_category, glueContext, 'store_category')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Load the data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://googleyelpproject/clean_data/stores\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"clean_data\", catalogTableName=\"stores\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(stores_dynamic)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f6fb1f0a650>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://googleyelpproject/clean_data/reviews\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"clean_data\", catalogTableName=\"reviews\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(reviews_dynamic)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f6fb1f0ab50>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://googleyelpproject/clean_data/clientes\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"clean_data\", catalogTableName=\"clientes\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(clientes_dynamic)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://googleyelpproject/clean_data/comentarios\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"clean_data\", catalogTableName=\"comentarios\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(comentarios_dynamic)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f6fb1f0afd0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://googleyelpproject/clean_data/states\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"clean_data\", catalogTableName=\"states\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(states_dynamic)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f6fb351c450>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://googleyelpproject/clean_data/category\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"clean_data\", catalogTableName=\"category\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(category_dynamic)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f6fb351c890>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://googleyelpproject/clean_data/store_category\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"clean_data\", catalogTableName=\"store_category\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(store_category_dynamic)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f6fb351cc10>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}