# INDATA NLP Web APP

## ‚ú® Indice

- [Introducci√≥n](#introduccion)
- [Uso](#modo-de-uso)
    - [Review Analysis](#review-analysis)
    - [Strengths and Weaknesses Analysis](#s-and-w)
- [Modelo de Machine Learning](#modelo-de-machine-learning)
    - [LDA](#LDA)
    - [BERT](#BERT)
- [Tecnolog√≠as utilizadas](#tecnologias)   
- [Contribuciones](#Contribuciones)

<br>
<p align=center><img width="80%" src="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/blob/main/src/indata_web_app_mainmenu.PNG"></p><br>


# **Introducci√≥n**
<div id='introduccion'/>
A fin de complementar el an√°lisis provisto en el Dashboard del proyecto, se desarroll√≥ una NLP Web App que puede ser consumida tanto por empresas que brinda atenci√≥n al cliente, tienen una firma de loclaes o usuarios finales.
Este sistema brinda dos funcionalidades principales:
1. Review Analysis: Esta primera funcionalidad toma una rese√±a (en idioma ingl√©s) y analiza si tiene una connotaci√≥n positiva, negativa o neutra. Adem√°s, analizar√° la preponderacia de distintos t√≥picos en la rese√±a.
2. Strengths and Weaknesses: En esta segunda funcionalidad, la aplicaci√≥n se centra en mostrar las fortalezas y debilidades de los distintos locales existentes en las plataformas consideradas en este proyecto. Pudiendo analizar lo que otros usuarios han remarcado como destacable o por mejorar en un local.


# **Modo de Uso**
<div id='modo-de-uso'/>

La Web App de INDATA se encuentra disponible en [Streamlit](https://indatanlpgastronomics.streamlit.app/).
Tal como lo presenta la App, podemos utilizarla de dos maneras.

## **Review Analysis**
<div id='review-analysis'/>
La primera como analizados de Topicos y Sentimientos en base a una rese√±a dada. Para ello indicamos en el men√∫ del sidebar **Review Analysis**. Luego, indicamos una rese√±a (en idioma ingl√©s) cuya connotaci√≥n y temas queremos obtener o analizar. Esperamos unos breves instantes luego de darle al bot√≥n de **Get Review** y la App nos mostrar√° el resultado.
En esta primera funalidad nos indicar√° si la rese√±a es positiva, negativa o neutra seguido de un gr√°fico de barras (bar plot) y un gr√°fico de torta (pie plot) mostrandonos la preponderancia de ciertos t√≥picos dentro de la rese√±a. En este caso, aquellos t√≥picos con mayor preponderaci ser√°n los m√°s destacables en
 la rese√±a.

<br>
<p align=center><img width="80%" src="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/blob/main/src/indata_web_app_menu.PNG"></p><br>

<br>
<p align=center><img width="80%" src="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/blob/main/src/indata_web_app_review_1.PNG"></p><br>

<br>
<p align=center><img width="80%" src="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/blob/main/src/indata_web_app_review_result.PNG"></p><br>


## **Strengths and Weaknesses Analysis**
<div id='s-and-w'/>
La segunda opci√≥n que nos brinda el men√∫ principal del sidebar es **Stores Strengths and Weaknesses**. En este caso, dada la inmensa cantidad de locales que existen en USA, la applicaci√≥n comienza realizando una proceso de filtrado guiado por y para el usuario. En primer instancia, en el seleccionamos el estado en **Choose your state**. Luego, seleccionamos alguna de las tantas catergor√≠as de local que tenemos disponibles dentro de ese estado en **Choose your category**. Una vez que hayamos elegido nuestro estado y nuestra categor√≠a, tendremos disponible un listado con todos los restaurants que entran dentro de ambas condiciones.

Una vez hayamos seleccionado nuestro restaurant de inter√©s en **Choose your Restaurant**, la aplicaci√≥n nos mostrar√°: Las principales fortalezas y debilidades del local (si existieran). Adem√°s, nos mostrar√° un Radar chart con los t√≥picos que los usuarios o clientes han mencionado anteriormente en rese√±as del local junto con su concepci√≥n promedio. Debajo del gr√°fico disponemos de una descripci√≥n de cada t√≥pico.


<br>
<p align=center><img width="80%" src="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/blob/main/src/indata_web_app_menu2.PNG"></p><br>

<br>
<p align=center><img width="80%" src="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/blob/main/src/indata_web_app_sw_result.PNG"></p><br>

<br>
<p align=center><img width="80%" src="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/blob/main/src/indata_web_app_review_result_topic.PNG"></p><br>

>**Nota:** Aquellos locales que tienen menos de 10 categorias es porque no hay registro de rese√±as para ese local que hayan mencionado ese aspecto.

# **Modelo de Machine Learning**
<div id='modelo-de-machine-learning'/>
Para nuestro modelo NLP hemos considerado dos bloques fundamentales: LDA para reconocimiento de tem√°ticas o topic modelling y BERT para analisis de sentimientos o sentiment analysis.

### <a href="https://github.com/volpo/PG_YELP_GOOGLE_MAPS/tree/main/ML_Model"> **LDA**
<div id='LDA'/>
LDA es un modelo probabil√≠stico utilizado en el campo del procesamiento del lenguaje natural para descubrir temas o t√≥picos subyacentes en un conjunto de documentos. El objetivo principal de LDA es asignar palabras a t√≥picos de manera autom√°tica, sin la necesidad de etiquetas o anotaciones previas.

El modelo LDA se basa en la idea de que un documento est√° compuesto por una mezcla de varios t√≥picos, y cada t√≥pico est√° caracterizado por una distribuci√≥n de palabras. La intuici√≥n detr√°s de LDA es que, si tenemos un conjunto de documentos y conocemos las palabras presentes en cada uno, podemos inferir los t√≥picos subyacentes m√°s probables en el corpus y la distribuci√≥n de palabras para cada t√≥pico.

El proceso de inferencia en LDA se realiza mediante el uso de t√©cnicas de aprendizaje autom√°tico y an√°lisis estad√≠stico. En resumen, el modelo LDA sigue los siguientes pasos:

1. Preparaci√≥n de datos: Primero, se realiza un preprocesamiento de los documentos, que puede incluir la eliminaci√≥n de palabras vac√≠as, la normalizaci√≥n del texto y la tokenizaci√≥n.

2. Construcci√≥n del modelo: Luego, se define el n√∫mero de t√≥picos que se desea extraer del corpus. Este n√∫mero puede ser determinado de antemano o explorado mediante t√©cnicas de validaci√≥n cruzada u otras medidas. Es decir, es un hiperpar√°metro que debemos tunear.

3. Entrenamiento del modelo: Una vez definido el n√∫mero de t√≥picos, se entrena el modelo LDA utilizando t√©cnicas de inferencia probabil√≠stica. Durante el entrenamiento, el modelo asigna palabras a t√≥picos y actualiza las distribuciones de palabras y t√≥picos iterativamente para maximizar la probabilidad de generar los documentos observados.

4. Inferencia de t√≥picos: Despu√©s de entrenar el modelo, se pueden inferir los t√≥picos m√°s probables para nuevos documentos o para documentos existentes en el corpus. Esto implica asignar una distribuci√≥n de t√≥picos a cada documento y una distribuci√≥n de palabras a cada t√≥pico.

5. Interpretaci√≥n de resultados: Finalmente, se pueden interpretar los resultados del modelo LDA examinando las distribuciones de palabras en cada t√≥pico y las asignaciones de t√≥picos a los documentos. Esto permite identificar y comprender los temas predominantes en el conjunto de documentos analizados. 

Notar que el modelo realiza un an√°lisis probabilistico, mas no nos devuelve el significado de cada t√≥pico identificado. Es labor nuestra determinarlo. Basandonos en un an√°lisis de coherencia, se determin√≥ que 10 t√≥picos era un n√∫mero que maximizaba la coherencia intra t√≥pico mientras que brindaba variedad de temas.
Luego se determinaron los siguientes significados para cada t√≥pico:

1. "Familiar"
2. "Negocio familiar - Peque√±o y Acogedor"
3. "Calidad de los postres"
4. "Variedad del men√∫"
5. "Tiempo de espera (tomar el pedido, responder preguntas y traer la comida)"
6. "Calidad de la comida (comida internacional)"
7. "Calidad de la comida (sabrosa y abundante)"
8. "Local tradicional"
9. "Ambiente nocturno divertido"
10. "Atenci√≥n al cliente, ambiente agradable y asequible"


### **BERT**
<div id='BERT'/>
El an√°lisis de sentimientos es una tarea fundamental en el procesamiento del lenguaje natural, y el modelo BERT (Bidirectional Encoder Representations from Transformers) ha demostrado ser una herramienta poderosa para abordar esta tarea. BERT se basa en la arquitectura de transformers, que es conocida por su capacidad para capturar relaciones de largo alcance en el texto.

El enfoque de BERT para el an√°lisis de sentimientos implica el pre-entrenamiento y el ajuste fino del modelo. En el pre-entrenamiento, BERT se entrena en grandes cantidades de texto sin etiquetas, lo que le permite aprender representaciones contextuales del lenguaje. Durante este proceso, el modelo adquiere un conocimiento general del lenguaje y captura sutilezas sem√°nticas y sint√°cticas.

Una vez que BERT ha sido pre-entrenado, se realiza un ajuste fino en un conjunto de datos espec√≠fico para el an√°lisis de sentimientos.
En este case, el ajuste fino se realiz√≥ en base a las reviews disponibles y la clasificaci√≥n o estrellas que el usuario daba al local como forma de obtener indirectamente los valores objetivos. Durante este proceso, el modelo ajusta sus pesos y aprende a asociar caracter√≠sticas del texto con las etiquetas de sentimiento correspondientes.

Una vez BERT ha sido entrenado con un conjunto de datos de an√°lisis de sentimientos, se puede utiliza para determinar si la connotaci√≥n de las rese√±as de los usuarios es positiva, negativa o neutra. 

# üõ†Ô∏è **Tecnolog√≠as**
<div id='tecnologias'/>

- Gensim
- HuggingFaces
- NLTK
- Pandas
- Python
- Spacy
- Streamlit


# Contribuciones
<div id='Contribuciones'/>
Si quieres contribuir en este proyecto no dudes en escribirme a m√≠ o a cualquiera de nuestro equipo.

Mi mail es carnaghi.marco@gmail.com estoy a disposici√≥n por cualquier duda que tengas.
